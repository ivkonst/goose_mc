{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of Notes from Downloaded Sites\n",
    "Form lists of phrases for each text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "import jsonlines\n",
    "import Levenshtein\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_soup(dd, link):\n",
    "    print('\\n'.join([abstract[0] for abstract in dd[link] if abstract[1] == 'text']))\n",
    "    \n",
    "def look_listphrases(listphrases):\n",
    "    print('\\n***********\\n'.join(['\\n'.join(paragraph) for paragraph in listphrases]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prohiphop.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "131it [00:00, 4429.90it/s]\n"
     ]
    }
   ],
   "source": [
    "dd_prohiphop = defaultdict(list)\n",
    "\n",
    "with jsonlines.open(os.path.join(data_dir,'site_soup_prohiphop.jsonl')) as fd:\n",
    "    for obj in tqdm(fd):\n",
    "        dd_prohiphop[obj['link']] += obj['soup']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts didn't download :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### webrap.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3152it [00:00, 5601.72it/s]\n"
     ]
    }
   ],
   "source": [
    "dd_webrap = defaultdict(list)\n",
    "\n",
    "with jsonlines.open(os.path.join(data_dir,'site_soup_webrap.jsonl')) as fd:\n",
    "    for obj in tqdm(fd):\n",
    "        dd_webrap[obj['link']] += obj['soup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n"
     ]
    }
   ],
   "source": [
    "list_text_uchastnikov_versusa = sorted([link for link in dd_webrap if re.match('^\\S*/text-uchastnikov-versusa\\S+$', link) != None])\n",
    "print(len(list_text_uchastnikov_versusa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1725\n"
     ]
    }
   ],
   "source": [
    "list_text_pesen = sorted([link for link in dd_webrap if re.match('^\\S*/text-pesen\\S+/\\S+$', link) != None])\n",
    "print(len(list_text_pesen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(soup):\n",
    "    list_text = []\n",
    "    list_tmp = []\n",
    "    fl_israp = 0\n",
    "    fl_newparagraph = 0\n",
    "    for abstract in soup:\n",
    "        fl_isphrase = 1\n",
    "        text = abstract[0]\n",
    "        block = abstract[2]\n",
    "        if re.match('^Просмотров: \\d+$', text) != None:\n",
    "            fl_israp = 1\n",
    "            fl_isphrase = 0\n",
    "        elif (fl_israp == 1) and ('a:' in block) and (len(list_text) > 0):\n",
    "            fl_israp = 0\n",
    "            fl_isphrase = 0\n",
    "        elif re.match('^\\S*h\\d:\\S*$', block) != None:\n",
    "            fl_isphrase = 0\n",
    "            fl_newparagraph = 1\n",
    "            \n",
    "        if (fl_israp == 1) and (fl_isphrase == 1):\n",
    "            if fl_newparagraph == 1:\n",
    "                if len(list_tmp) > 0:\n",
    "                    list_text.append(list_tmp)\n",
    "                    list_tmp = [text]\n",
    "                else:\n",
    "                    list_tmp = [text]\n",
    "                fl_newparagraph = 0\n",
    "            else:\n",
    "                list_tmp.append(text)\n",
    "    if len(list_tmp) > 0:\n",
    "        list_text.append(list_tmp)\n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 247/247 [00:00<00:00, 1512.97it/s]\n"
     ]
    }
   ],
   "source": [
    "dd_text_uchastnikov_versusa = {link:extraction(dd_webrap[link]) for link in tqdm(list_text_uchastnikov_versusa)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction(soup):\n",
    "    list_text = []\n",
    "    list_tmp = []\n",
    "    fl_israp = 0\n",
    "    fl_newparagraph = 0\n",
    "    num_paragraph = -999\n",
    "    for abstract in soup:\n",
    "        fl_isphrase = 1\n",
    "        text = abstract[0]\n",
    "        block = abstract[2]\n",
    "        if re.match('^Просмотров: \\d+$', text) != None:\n",
    "            fl_israp = 1\n",
    "            fl_isphrase = 0\n",
    "        elif 'a:' in block:\n",
    "            fl_israp = 0\n",
    "            fl_isphrase = 0\n",
    "        elif ('припев' in text.lower()) and (len(text) <= 20):\n",
    "            fl_isphrase = 0\n",
    "            \n",
    "        if re.match('^\\S*p:\\S*$', block) != None:\n",
    "            num_paragraph_curr = int(re.findall(r'p:\\d+', block)[0].split(':')[1])\n",
    "            if num_paragraph_curr > num_paragraph:\n",
    "                num_paragraph = num_paragraph_curr\n",
    "                fl_newparagraph = 1\n",
    "        \n",
    "        if (fl_israp == 1) and (fl_isphrase == 1):\n",
    "            if fl_newparagraph == 1:\n",
    "                if len(list_tmp) > 0:\n",
    "                    list_text.append(list_tmp)\n",
    "                    list_tmp = [text]\n",
    "                else:\n",
    "                    list_tmp = [text]\n",
    "                fl_newparagraph = 0\n",
    "            else:\n",
    "                list_tmp.append(text)\n",
    "    if len(list_tmp) > 0:\n",
    "        list_text.append(list_tmp)\n",
    "    \n",
    "    if len(list_text) >= 10:\n",
    "        list_text = [[phrase for paragraph in list_text for phrase in paragraph]]\n",
    "    \n",
    "    return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1725/1725 [00:00<00:00, 2062.57it/s]\n"
     ]
    }
   ],
   "source": [
    "dd_text_pesen = {link:extraction(dd_webrap[link]) for link in tqdm(list_text_pesen)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1972\n"
     ]
    }
   ],
   "source": [
    "dd_webrap_texts = dd_text_uchastnikov_versusa.copy()\n",
    "\n",
    "for link, list_text in dd_text_pesen.items():\n",
    "    dd_webrap_texts[link] = list_text\n",
    "print(len(dd_webrap_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(os.path.join(data_dir, 'texts_webrap.jsonl'), mode='w') as fd:\n",
    "    for link, list_texts in dd_webrap_texts.items():\n",
    "        if len(list_texts) > 0:\n",
    "            fd.write({'link':link, 'list_texts':list_texts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repair Words with Star\n",
    "(ex.: *ж*па* -> *жопа*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1971it [00:21, 93.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with star 1325\n",
      "Number of words without star 97895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_word_w_star = defaultdict(int)\n",
    "dict_word_wo_star = defaultdict(int)\n",
    "\n",
    "with jsonlines.open(os.path.join(data_dir, 'texts_webrap.jsonl')) as fd:\n",
    "    for obj in tqdm(fd):\n",
    "        for list_phrase in obj['list_texts']:\n",
    "            for phrase in list_phrase:\n",
    "                toks = [tok for tok in nltk.word_tokenize(phrase.lower()) \\\n",
    "                        if len(tok) > 1 and re.sub('[^a-zа-я*]', '', tok) != '']\n",
    "                for tok in toks:\n",
    "                    if '*' in tok:\n",
    "                        if len(tok) == (len(re.sub('[*]', '', tok)) + 1):\n",
    "                            dict_word_w_star[tok] += 1\n",
    "                    else:\n",
    "                        dict_word_wo_star[tok] += 1\n",
    "                        \n",
    "print('Number of words with star', len(dict_word_w_star))\n",
    "print('Number of words without star', len(dict_word_wo_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('бл*ть', 400),\n",
       " ('х*й', 340),\n",
       " ('нах*й', 274),\n",
       " ('пох*й', 150),\n",
       " ('бл*дь', 96),\n",
       " ('на*уй', 81),\n",
       " ('*бал', 79),\n",
       " ('еб*ть', 78),\n",
       " ('п*здец', 76),\n",
       " ('с*ка', 65)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict_word_w_star.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_word_wo_star['блять']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_word_w_star = [word for word, count in sorted(dict_word_w_star.items(), key=lambda x: x[1], reverse=True)]\n",
    "list_word_wo_star = [word for word, count in sorted(dict_word_wo_star.items(), key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "def repair_word(word_w_s, list_wo_s=list_word_wo_star, dist=1):\n",
    "    len_word_w_s = len(word_w_s)\n",
    "    for word_wo_s in list_wo_s:\n",
    "        if len_word_w_s == len(word_wo_s) and Levenshtein.distance(word_w_s, word_wo_s) == dist:\n",
    "            return word_wo_s\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'блять'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repair_word('бл*ть')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1325/1325 [00:13<00:00, 95.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of repaired words 761 [('бл*ть', 'блять'), ('х*й', 'хуй'), ('нах*й', 'нахуй'), ('пох*й', 'похуй'), ('бл*дь', 'блядь'), ('на*уй', 'нахуй'), ('*бал', 'ебал'), ('еб*ть', 'ебать'), ('п*здец', 'пиздец'), ('с*ка', 'сука')]\n",
      "Number of not repaired words 564 ['на*хуй', 'х*есос', 'х*еплет', 'ох*евали', 'долбо*ба', 'п*здобол', 'у*бал', 'п*дараса', 'раз*бал', 'по*балу']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_word_repaired = {}\n",
    "list_word_notrepaired = []\n",
    "for word_w_s in tqdm(list_word_w_star):\n",
    "    word_wo_s = repair_word(word_w_s)\n",
    "    if word_wo_s is not None:\n",
    "        dict_word_repaired[word_w_s] = word_wo_s\n",
    "    else:\n",
    "        list_word_notrepaired.append(word_w_s)\n",
    "\n",
    "print('Number of repaired words', len(dict_word_repaired), list(dict_word_repaired.items())[:10])\n",
    "print('Number of not repaired words', len(list_word_notrepaired), list_word_notrepaired[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'dict_wws.json'), 'w') as fd:\n",
    "    fd.write(json.dumps(dict_word_repaired))\n",
    "    \n",
    "# with open(os.path.join(data_dir, 'dict_wws.json')) as fd:\n",
    "#     dict_word_repaired = json.load(fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_char_filt = defaultdict(int)\n",
    "\n",
    "with jsonlines.open(os.path.join(data_dir, 'texts_webrap.jsonl')) as fd:\n",
    "    for obj in fd:\n",
    "        list_texts = obj['list_texts']\n",
    "        for list_text in list_texts:\n",
    "            for phrase in list_text:\n",
    "                for char in re.sub('''[^!\"()*,-.:;? 0-9A-Za-zА-Яа-яЁё]''','',phrase):\n",
    "                    dict_char_filt[char] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ё', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Ъ', 'Ы', 'Ь', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n"
     ]
    }
   ],
   "source": [
    "print([char for char, freq in sorted(dict_char_filt.items(), key=lambda x: x[0])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
